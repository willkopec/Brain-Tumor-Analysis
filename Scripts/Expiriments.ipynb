{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEC6WS1GKg9D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# Expiriments\n",
    "#\n",
    "# Loads in all the tumor images and runs the expiriments on the  CNNs\n",
    "#\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import (Conv2D, MaxPool2D, Flatten, Dense, Dropout, SpatialDropout2D)\n",
    "\n",
    "\n",
    "PREDICTIONS_FILE = 'C:/Users/William/Desktop/CS 415 Project/WillKopec.pk1'\n",
    "\n",
    "\n",
    "###########################\n",
    "# Simple CNN Model\n",
    "def create_model_Simple_CNN(num_classes, input_shape):\n",
    "  model_Channel_Stacked = Sequential()\n",
    "  model_Channel_Stacked.add(Conv2D(6, kernel_size = (5, 5), activation='relu', padding='same',\n",
    "                input_shape=input_shape, kernel_initializer='random_normal'))\n",
    "  model_Channel_Stacked.add(MaxPool2D((2,2)))\n",
    "\n",
    "  model_Channel_Stacked.add(Conv2D(3, kernel_size = (10, 10), activation='relu', \n",
    "                padding='same', kernel_initializer='random_normal'))\n",
    "\n",
    "  model_Channel_Stacked.add(Flatten())\n",
    "  model_Channel_Stacked.add(Dense(100, activation='relu', kernel_initializer='random_normal'))\n",
    "  model_Channel_Stacked.add(Dropout(0.5))\n",
    "  model_Channel_Stacked.add(Dense(num_classes, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "\n",
    "  model_Channel_Stacked.compile(tf.keras.optimizers.Adam(0.0001), \n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "  return model_Channel_Stacked\n",
    "\n",
    "def run_trial(X, Y, model, prediction_file):\n",
    "  from sklearn.model_selection import StratifiedShuffleSplit\n",
    "  from keras.callbacks import EarlyStopping\n",
    "\n",
    "  results = {}\n",
    "  model.save_weights('checkpoints/my_checkpoint')\n",
    "\n",
    "  sss = StratifiedShuffleSplit(n_splits=3, random_state=0)\n",
    "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "\n",
    "  indices = []\n",
    "  predictions = []\n",
    "  labels = []\n",
    "\n",
    "  # Runs 3 iterations of a 5 fold cross validation\n",
    "  for iteration in range(3):\n",
    "    for train_index, test_index in sss.split(X, Y): # gets the train and test indecies\n",
    "      model.load_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "      train_index.astype(int, copy = False)\n",
    "      test_index.astype(int, copy = False)\n",
    "\n",
    "      X_train, X_test = X[train_index], X[test_index]\n",
    "      y_train, y_test = Y[train_index], Y[test_index]\n",
    "  \n",
    "      model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 1000, batch_size = 50, callbacks = [es], verbose = 0)\n",
    "      fold_predictions = model.predict(X_test)\n",
    "\n",
    "      # saves the predictions\n",
    "      indices += X_test.tolist()\n",
    "      predictions += fold_predictions.tolist()\n",
    "      labels += y_test.tolist()\n",
    "\n",
    "  results['indices'] = indices\n",
    "  results['predictions'] = predictions\n",
    "  results['labels'] = labels\n",
    "\n",
    "  open(prediction_file, 'wb').close()\n",
    "  outfile = open(prediction_file, 'wb')\n",
    "\n",
    "\n",
    "def create_dataset(path):\n",
    "  dataset_dictionary = pickle.load(open(path, 'rb'))\n",
    "\n",
    "  X, Y = [], []\n",
    "\n",
    "  X_0 = np.array([x for x in dataset_dictionary['1'] if x.shape == (512, 512)])\n",
    "  #X_1 = np.array([x for x in dataset_dictionary['2'] if x.shape == (512, 512)])\n",
    "  #X_2 = np.array([x for x in dataset_dictionary['3'] if x.shape == (512, 512)])\n",
    "\n",
    "  Y += [0] * X_0.shape[0]\n",
    "  #Y += [1] * X_1.shape[0]\n",
    "  #Y += [2] * X_2.shape[0]\n",
    "\n",
    "  X = np.vstack([X_0])\n",
    "  Y = np.array(Y)\n",
    "\n",
    "  return X, Y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  ########################\n",
    "  # Runs the trial with the simple crop features\n",
    "  X, Y = create_dataset('C:/Users/William/Desktop/CS 415 Project/Preprocessed Data/Vanilla Crop/WillKopec.pk1')\n",
    "  X = X.reshape((X.shape[0], X.shape[1], X.shape[2], 1))\n",
    "  print(X.shape)\n",
    "  model = create_model_Simple_CNN(num_classes = 3, input_shape = (X.shape[1], X.shape[2], 1))\n",
    "  run_trial(X, Y, model, PREDICTIONS_FILE)\n",
    "  print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Expiriments.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
